#Computer 
The bit is the most basic unit of [[definition]] in computing and [[digital]] [[communication]]s. The name is a portmanteau of [[binary]] [[digit]].[1] The bit represents a [[logic]]al state with one of two possible values. These values are most commonly represented as either "[[1]]" or "[[0]]", but other representations such as [[true]]/[[false]], [[yes]]/[[no]], [[on]]/[[off]], or [[+]]/[[âˆ’]] are also widely used.

The relation between these values and the [[physic]]al states of the underlying storage or [[device]] is a matter of convention, and different assignments may be used even within the same device or [[program]]. It may be physically implemented with a [[two-state]] device.

A contiguous group of binary digits is commonly called a [[bit string]], a [[bit vector]], or a [[single-dimensional]] (or [[multi-dimensional]]) [[bit array]]. A group of eight bits is called one [[byte]], but historically the size of the byte is not strictly defined.[2] Frequently, half, full, double and quadruple words consist of a number of bytes which is a low power of two. A string of four bits is a [[nibble]].

In information theory, one bit is the information [[entropy]] of a random [[binary]] variable that is 0 or 1 with equal probability,[3] or the information that is gained when the value of such a variable becomes known.[4][5] As a unit of information, the bit is also known as a shannon,[6] named after Claude E. Shannon.

The symbol for the binary digit is either "bit" as per the [[IEC]] 80000-13:2008 standard, or the lowercase character "b", as per the IEEE 1541-2002 standard. Use of the latter may create confusion with the capital "B" which is the international standard symbol for the byte.